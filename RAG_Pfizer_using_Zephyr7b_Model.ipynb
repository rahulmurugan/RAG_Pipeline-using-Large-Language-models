{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading patents"
      ],
      "metadata": {
        "id": "LWP7R4Ec9axW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install wget\n",
        "%pip install html5lib\n",
        "%pip install requests\n",
        "%pip install bs4"
      ],
      "metadata": {
        "id": "6Wd4eoM58-6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the csv containing links to patents\n",
        "import pandas as pd\n",
        "patent_list = pd.read_csv('specify path to csv containing patent links')\n",
        "urls_df = patent_list['result link']"
      ],
      "metadata": {
        "id": "9pEVBKDM9Xqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7mwMhLzrX_tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all links to pages in english\n",
        "# Function to convert the last two letters to \"en\"\n",
        "def convert_to_en(link):\n",
        "    if len(link) >= 2:\n",
        "        modified_link = link[:-2] + \"en\"\n",
        "        return modified_link\n",
        "    else:\n",
        "        return link\n",
        "\n",
        "mod_urls = [convert_to_en(url) for url in urls_df]"
      ],
      "metadata": {
        "id": "XpUTnH-n97Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_urls_df = pd.DataFrame(mod_urls)"
      ],
      "metadata": {
        "id": "WL9DKT3S-UVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import linesep\n",
        "import requests\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "for i in tqdm(range(len(mod_urls_df))):\n",
        "  response = requests.get(mod_urls_df.iloc[i].values[0])\n",
        "  html = BeautifulSoup(response.content, 'html5lib')\n",
        "  p = html.find_all(\"div\")\n",
        "  pDup = []\n",
        "  for element in p:\n",
        "    if not element.table:\n",
        "      text = re.sub(r'[^a-zA-z0-9' '\\n\\.]', ' ', element.text)\n",
        "      text = re.sub(r'[\\n]{1,}', ' ', text)\n",
        "      pDup.append(re.sub(r\"[' ']+\", ' ', text))\n",
        "  with open(f'D:/Research/Pfizer_project/corpus/{i}.txt', 'w') as f:\n",
        "    for lines in pDup:\n",
        "      f.write(lines)"
      ],
      "metadata": {
        "id": "yvBDr64u-U7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG framework"
      ],
      "metadata": {
        "id": "7vJ2C_yB_DsY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88QbKIGdAel8"
      },
      "outputs": [],
      "source": [
        "%pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "%pip install langchain tiktoken chromadb InstructorEmbedding\n",
        "%pip install accelerate loralib bitsandbytes sentencepiece xformers einops\n",
        "%pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from IPython.display import Markdown, display\n",
        "import chromadb\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "C8IB_dTEAjtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the LLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=\"auto\", device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=\"auto\")"
      ],
      "metadata": {
        "id": "_nisc0o9Am8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_directory = '/mnt/Senai_grp/ArunM/Pfizer_project/corpu'\n",
        "loader = DirectoryLoader(corpus_directory, glob=\"*.txt\", loader_cls=TextLoader)\n",
        "documents = loader.load()\n",
        "\n",
        "# Splitting text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Get the number of text chunks\n",
        "num_chunks = len(texts)\n",
        "print(f\"Number of text chunks: {num_chunks}\")"
      ],
      "metadata": {
        "id": "e1HKe66oAsfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download embedding model\n",
        "\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "model_name = \"BAAI/bge-large-en\"\n",
        "encode_kwargs = {'normalize_embeddings': True}  # set True to compute cosine similarity\n",
        "\n",
        "model_norm = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "O9QI2oQdBI2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = 'Specify the path of directory to store the embeddings' # chunksize = 4096\n",
        "\n",
        "# Making a vector database\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                embedding=model_norm,\n",
        "                                persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "So7kgpXHB_c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If loading from existing vector database\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=model_norm)"
      ],
      "metadata": {
        "id": "7jDWQaiMCKIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the retriever\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "3vpJwtdoCTAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to process response and return the source\n",
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        # print(source.metadata['source'])\n",
        "        return(source.metadata['source'])"
      ],
      "metadata": {
        "id": "wLXjxsnXCXoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create wrokflow for LLM\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length = 5000,\n",
        "    temperature=0.0001,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.35\n",
        ")\n",
        "local_llm = HuggingFacePipeline(pipeline = pipe)\n",
        "\n",
        "# Create RAG chain\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=local_llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "S0hp5eeBCdLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter query\n",
        "query = \"Mention important attributes of a drug release profile?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "H2I8QjN-C0Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing question from file"
      ],
      "metadata": {
        "id": "HUkh2_SzC8Hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = 'Enter path of input text file containing one question per line'\n",
        "output_file_path = 'Enter path of output file to write questions and responses'"
      ],
      "metadata": {
        "id": "FEDxyQnhC-1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(input_file_path, 'r') as input_file, open(output_file_path, 'w') as output_file:\n",
        "    question_number = 1  # Initialize the question number\n",
        "\n",
        "    # Loop through each line in the input file\n",
        "    for line in input_file:\n",
        "        # Remove leading and trailing whitespaces\n",
        "        question = line.strip()\n",
        "\n",
        "        # Send the question to the qa_chain function to get a response\n",
        "        llm_response = qa_chain(question)\n",
        "        # processed_response = process_llm_response(llm_response)\n",
        "\n",
        "        # Write the question and response to the output file\n",
        "        # output_file.write(f\"Question {question_number}: {question}\\n\")\n",
        "        output_file.write(f\"Question: {question}\\n\")\n",
        "        output_file.write(\"Response:\\n\")\n",
        "        output_file.write(wrap_text_preserve_newlines(llm_response['result']) + '\\n')\n",
        "        output_file.write('\\nSources:\\n')\n",
        "\n",
        "        # Loop through the source documents in the response\n",
        "        for source in llm_response[\"source_documents\"]:\n",
        "            output_file.write(source.metadata['source'] + '\\n')\n",
        "\n",
        "        output_file.write('\\n\\n')  # Separate each question-response pair\n",
        "\n",
        "        question_number += 1  # Increment the question number\n",
        "\n",
        "print(\"Questions and responses written to the output file.\")"
      ],
      "metadata": {
        "id": "90sdY1teDLpf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}