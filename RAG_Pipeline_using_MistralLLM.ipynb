{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWBmGtOrs2hV"
      },
      "outputs": [],
      "source": [
        "%pip install langchain tiktoken chromadb pypdf transformers InstructorEmbedding\n",
        "%pip install accelerate bitsandbytes sentencepiece Xformers\n",
        "%pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "HlR-GoZys4xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")"
      ],
      "metadata": {
        "id": "TJwEjAPfs6-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "import chromadb"
      ],
      "metadata": {
        "id": "zOKrSZvhs9M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_directory = '/mnt/Senai_grp/RahulM/pfizerproj/bin/corpus_test 2'\n",
        "loader = DirectoryLoader(corpus_directory, glob=\"*.txt\", loader_cls=TextLoader)\n",
        "documents = loader.load()\n",
        "\n",
        "# Splitting text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Get the number of text chunks\n",
        "num_chunks = len(texts)\n",
        "\n",
        "print(f\"Number of text chunks: {num_chunks}\")"
      ],
      "metadata": {
        "id": "ajmTBCw_s_SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "model_name = \"BAAI/bge-base-en\"\n",
        "encode_kwargs = {'normalize_embeddings': True}  # set True to compute cosine similarity\n",
        "\n",
        "model_norm = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "#  Use the BGE embeddings to encode text chunks (texts) and perform further tasks\n",
        "#  Example: Get embeddings for text chunks\n",
        "# embeddings = model_norm.encode(texts)"
      ],
      "metadata": {
        "id": "L2aIMJBYtBaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = 'db'\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                embedding=model_norm,\n",
        "                                persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "7wld-jJ5tFsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "lpRIQbLVtHfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import torch\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=2048,\n",
        "    temperature=0,\n",
        "    # do_sample=True,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15\n",
        ")\n",
        "local_llm = HuggingFacePipeline(pipeline = pipe)"
      ],
      "metadata": {
        "id": "p8FI-67VtJKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(llm=local_llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "b41bxrY6tKCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ],
      "metadata": {
        "id": "RInT9R6ttN2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Examples of useful water soluble excipients in the present invention includes?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "gIEbAxmjtQE1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}